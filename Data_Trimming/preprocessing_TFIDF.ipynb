{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing_TFIDF","provenance":[{"file_id":"1lA_G11CLzCdHdY3sfXdmISaVwIDwPf15","timestamp":1575146800182},{"file_id":"1Nhq2MohlePHJ-FpZWsKP8BI8MZkEBNAJ","timestamp":1575145814698}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"BqpW0ZHo7d_X","colab_type":"code","colab":{}},"source":["import json\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FMWwQj0lCZqM","colab_type":"code","outputId":"49b315ca-c434-49ec-a8ea-87c62c30d879","executionInfo":{"status":"ok","timestamp":1575149994587,"user_tz":360,"elapsed":18190,"user":{"displayName":"Daniel Tang","photoUrl":"","userId":"09540227816050740255"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yEFiNGrj8AML","colab_type":"code","colab":{}},"source":["root_path = '/content/drive/My Drive/DSCI 303 Final Project/Data Trimming/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"syimYoiu7jAc","colab_type":"code","colab":{}},"source":["with open(root_path + 'trimmed_review_150k.json') as json_file: # Open json file to load\n","    data = json.load(json_file) # Load the data as a big dictionary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5heRpu637kGe","colab_type":"code","colab":{}},"source":["df = pd.DataFrame(data['Review:Rating']) # Convert list of dictionaries {review, stars} to dataframe"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ZSVrS6P93nP","colab_type":"code","colab":{}},"source":["# df['review_length'] = df['review'].apply(lambda x: len(x.split(\" \"))) # Get review lenth in words as a features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HTMS94S8QvHe","colab_type":"code","colab":{}},"source":["def convert_lower_case(data):\n","    return np.char.lower(data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"spZSleQqRhJs","colab_type":"code","outputId":"e2ef32b7-a44d-4c59-bca2-83830fd5919d","executionInfo":{"status":"ok","timestamp":1575150403368,"user_tz":360,"elapsed":840,"user":{"displayName":"Daniel Tang","photoUrl":"","userId":"09540227816050740255"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"4aXDIU_hQ0Xq","colab_type":"code","colab":{}},"source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","def remove_stop_words(data):\n","  \"\"\"\n","  Input: str\n","  Output: str\n","  Removes stop words like I, me, the, etc. For preprocessing the data\n","  \"\"\"\n","  stop_words = stopwords.words('english')\n","  words = word_tokenize(str(data))\n","  new_text = \"\"\n","  for w in words:\n","      if w not in stop_words and len(w) > 1:\n","          new_text = new_text + \" \" + w\n","  return new_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xenOp-jLS88A","colab_type":"code","colab":{}},"source":["def remove_punctuation(data):\n","  \"\"\"\n","  Input: str\n","  Output: str\n","  Further preprocessing\n","  \"\"\"\n","  symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n","  for i in range(len(symbols)):\n","      data = np.char.replace(data, symbols[i], ' ')\n","      data = np.char.replace(data, \"  \", \" \")\n","  data = np.char.replace(data, ',', '')\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JsHbFq-aTB5I","colab_type":"code","colab":{}},"source":["def remove_apostrophe(data):\n","  \"\"\"\n","  Input: str\n","  Output: str\n","  Further preprocessing\n","  \"\"\"\n","  return np.char.replace(data, \"'\", \"\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UU2GtnsDUiNu","colab_type":"code","colab":{}},"source":["from nltk.stem import PorterStemmer\n","def stemming(data):\n","  \"\"\"\n","  Input: str\n","  Output: str\n","  Converts words to their stem. Ex: worked -> work. Removes suffix and affix. No need for lemmatization for TFIDF\n","  \"\"\"\n","  stemmer= PorterStemmer()\n","  \n","  tokens = word_tokenize(str(data))\n","  new_text = \"\"\n","  for w in tokens:\n","      new_text = new_text + \" \" + stemmer.stem(w)\n","  return new_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0D7NcXTWJ7H","colab_type":"code","outputId":"73a66d8f-0d2d-4cc5-ec3d-684cc54b70a3","executionInfo":{"status":"ok","timestamp":1575151522738,"user_tz":360,"elapsed":4149,"user":{"displayName":"Daniel Tang","photoUrl":"","userId":"09540227816050740255"}},"colab":{"base_uri":"https://localhost:8080/","height":138}},"source":["!pip install num2words"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting num2words\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n","\r\u001b[K     |███▎                            | 10kB 33.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 2.3MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n","Installing collected packages: num2words\n","Successfully installed num2words-0.5.10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ItlGQV9uV3Bs","colab_type":"code","colab":{}},"source":["from num2words import num2words\n","def convert_numbers(data):\n","    tokens = word_tokenize(str(data))\n","    new_text = \"\"\n","    for w in tokens:\n","        try:\n","            w = num2words(int(w))\n","        except:\n","            a = 0\n","        new_text = new_text + \" \" + w\n","    new_text = np.char.replace(new_text, \"-\", \" \")\n","    return new_text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-8iBFfWZaHir","colab_type":"text"},"source":["**Pipeline** below for preprocessing our text in our dataframe using the functions above:"]},{"cell_type":"code","metadata":{"id":"gFg8K7H3Q6Ii","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: convert_lower_case(x)) #Convert each review to lowercase"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-wcZaZwSjdZ","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: remove_punctuation(x)) #Remove punctuation from each review"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRqEMA5lW4UX","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: remove_apostrophe(x)) #Remove apostrophes from each review"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"li0CYXZZXUYf","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: remove_stop_words(x)) #Remove stop words from each review"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcjP5mrcXgVJ","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: convert_numbers(x)) #Convert numerics to string equivalents"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRLaKDriXq9l","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: stemming(x)) #Stem all the words from each review"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPF2YD7iX-yn","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: remove_punctuation(x)) #Repeated just in case punctuation was reintroduced"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PTRUYy3Yxvu","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: convert_numbers(x)) #Just in case more numbers were reintroduced"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ECnwj1cfY8CV","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: stemming(x)) #Just in case numbers needed to be stemmed again "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-gZ3f_oZk6h","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: remove_punctuation(x)) #Repeated because num2words does give some hyphens and commas"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-N0MvL6Z1G4","colab_type":"code","colab":{}},"source":["df['review'] = df['review'].apply(lambda x: remove_stop_words(x)) #Repeated because num2words does give stop words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dml9PZxT_B7Q","colab_type":"code","outputId":"da14ead1-a905-4eea-e2be-61c24d524e9a","executionInfo":{"status":"ok","timestamp":1575152745045,"user_tz":360,"elapsed":282,"user":{"displayName":"Daniel Tang","photoUrl":"","userId":"09540227816050740255"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["df.head(6)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>stars</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>total bill horribl servic 8g crook actual ner...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ador travi hard rock new kelli cardena salon ...</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>say offic realli togeth organ friendli dr phi...</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>went lunch steak sandwich delici caesar salad...</td>\n","      <td>5.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>today second three session paid although firs...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ill first admit excit go la tavolta food snob...</td>\n","      <td>4.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  stars\n","0   total bill horribl servic 8g crook actual ner...    1.0\n","1   ador travi hard rock new kelli cardena salon ...    5.0\n","2   say offic realli togeth organ friendli dr phi...    5.0\n","3   went lunch steak sandwich delici caesar salad...    5.0\n","4   today second three session paid although firs...    1.0\n","5   ill first admit excit go la tavolta food snob...    4.0"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"HlPqblE-be23","colab_type":"code","colab":{}},"source":["data = df.to_json(orient='records')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rleD7qt-b3dQ","colab_type":"code","colab":{}},"source":["with open(root_path + \"preprocessed_150k_reviews.json\", 'w') as outfile:\n","    json.dump(data, outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmnxUatN6LST","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0085919a-1148-4d72-d103-e6a7d1a094e3","executionInfo":{"status":"ok","timestamp":1575161011179,"user_tz":360,"elapsed":4365,"user":{"displayName":"Daniel Tang","photoUrl":"","userId":"09540227816050740255"}}},"source":[""],"execution_count":2,"outputs":[{"output_type":"stream","text":["sample_data\n"],"name":"stdout"}]}]}